{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing import read_lexical_corpus\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train ...\n",
      "Processing trial ...\n",
      "Processing test ...\n",
      "Processing train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krivas/projects/lexical_augmentation/src/preprocessing.py:115: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(texts), np.array(corpus), np.array(labels), np.array(sentence_raw), np.array(target_words), np.array(positions), np.array(pos_tag_sentences), np.array(pos_tag_targets)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trial ...\n",
      "Processing test ...\n"
     ]
    }
   ],
   "source": [
    "window_data = {}\n",
    "window_df_data = {}\n",
    "\n",
    "for window_x in [0, 1]:\n",
    "    print('Processing train ...')\n",
    "    train_texts, data_corpus, train_labels, sentence_train, train_target_words, train_positions, a, train_pos_tags = read_lexical_corpus('data/raw/lcp_single_train.tsv',\n",
    "                                        nlp=nlp,\n",
    "                                        return_complete_sent=False,\n",
    "                                        window_size=window_x + 1)\n",
    "\n",
    "    print('Processing trial ...')\n",
    "    trial_texts, trial_corpus, trial_labels, sentences_trial, trial_target_words, trial_positions, b, trial_pos_tags = read_lexical_corpus('data/raw/lcp_single_trial.tsv',\n",
    "                                        nlp=nlp,\n",
    "                                        return_complete_sent=False,\n",
    "                                        window_size=window_x + 1)\n",
    "    \n",
    "    print('Processing test ...')\n",
    "    test_texts, test_corpus, test_labels, sentences_test, test_target_words, test_positions, c, test_pos_tags = read_lexical_corpus('data/raw/lcp_single_test.tsv',\n",
    "                                        nlp=nlp,\n",
    "                                        return_complete_sent=False,\n",
    "                                        window_size=window_x + 1)\n",
    "    \n",
    "    multi_texts, multi_corpus, multi_labels, sentence_multi, multi_target_words, multi_positions, d, multi_pos_tags = read_lexical_corpus('data/raw/lcp_multi_train.tsv',\n",
    "                                        nlp=nlp,\n",
    "                                        return_complete_sent=False,\n",
    "                                        window_size=window_x + 1)\n",
    "\n",
    "    window_data[window_x] = {'train': [], 'train_multi': [], 'val': [], 'test': []}\n",
    "    window_data[window_x]['train'] = [train_texts, data_corpus, train_labels, sentence_train, train_target_words, train_positions, a, train_pos_tags]\n",
    "    window_data[window_x]['val'] = [trial_texts, trial_corpus, trial_labels, sentences_trial, trial_target_words, trial_positions, b, trial_pos_tags]\n",
    "    window_data[window_x]['test'] = [test_texts, test_corpus, test_labels, sentences_test, test_target_words, test_positions, c, test_pos_tags]\n",
    "    \n",
    "    window_data[window_x]['train_multi'] = [multi_texts, multi_corpus, multi_labels, sentence_multi, multi_target_words, multi_positions, d, multi_pos_tags]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing import read_cwi_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train ...\n",
      "Processing trial ...\n",
      "Processing test ...\n",
      "Processing train ...\n",
      "Processing trial ...\n",
      "Processing test ...\n"
     ]
    }
   ],
   "source": [
    "window_data_cwi = {}\n",
    "\n",
    "for window_x in [0, 1]:\n",
    "    print('Processing train ...')\n",
    "    news_texts, news_labels, news_target_words, news_positions, news_pos_tags = read_cwi_corpus('data/extra/traindevset/english/News_Train.tsv',\n",
    "                                        nlp=nlp,\n",
    "                                        return_complete_sent=False,\n",
    "                                        window_size=window_x + 1)\n",
    "\n",
    "    print('Processing trial ...')\n",
    "    wikipedia_text, wikipedia_labels, wikipedia_target_words, wikipedia_positions, wikipedia_pos_tags = read_cwi_corpus('data/extra/traindevset/english/Wikipedia_Train.tsv',\n",
    "                                        nlp=nlp,\n",
    "                                        return_complete_sent=False,\n",
    "                                        window_size=window_x + 1)\n",
    "    \n",
    "    print('Processing test ...')\n",
    "    wikinew_texts, wikinew_labels, wikinew_target_words, wikinew_positions, wikinew_pos_tags = read_cwi_corpus('data/extra/traindevset/english/WikiNews_Train.tsv',\n",
    "                                        nlp=nlp,\n",
    "                                        return_complete_sent=False,\n",
    "                                        window_size=window_x + 1)\n",
    "    \n",
    "\n",
    "    window_data_cwi[window_x] = {'news': [], 'wikipedia': [], 'wikinews': []}\n",
    "    window_data_cwi[window_x]['news'] = [news_texts, news_labels, news_target_words, news_positions, news_pos_tags]\n",
    "    window_data_cwi[window_x]['wikipedia'] = [wikipedia_text, wikipedia_labels, wikipedia_target_words, wikipedia_positions, wikipedia_pos_tags]\n",
    "    window_data_cwi[window_x]['wikinews'] = [wikinew_texts, wikinew_labels, wikinew_target_words, wikinew_positions, wikinew_pos_tags]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('window_data.pkl', 'wb') as handle:\n",
    "    pickle.dump(window_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('window_data_cwi.pkl', 'wb') as handle:\n",
    "    pickle.dump(window_data_cwi, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "#with open('results_window.pkl', 'rb') as handle:\n",
    "#    results_window = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
