{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "os.chdir('..')\n",
    "with open('window_data.pkl', 'rb') as handle:\n",
    "    window_data = pickle.load(handle)\n",
    "    \n",
    "with open('window_data_cwi.pkl', 'rb') as handle:\n",
    "    window_data_cwi = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "import pandas as pd\n",
    "import csv\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "df_true = pd.read_csv('single_labels.tsv', sep='\\t', quoting=csv.QUOTE_NONE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('data/raw/lcp_single_test.tsv', sep='\\t', quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window size of  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 6 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.018231\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.068799\n",
      "  Validation Loss: 0.008063\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.072310\n",
      "  Validation Loss: 0.009061\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.699603\n",
      "  Validation Loss: 0.496928\n",
      "\n",
      "======== Epoch 2 / 6 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.011693\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.065894\n",
      "  Validation Loss: 0.007115\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.070618\n",
      "  Validation Loss: 0.008538\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.680703\n",
      "  Validation Loss: 0.474111\n",
      "\n",
      "======== Epoch 3 / 6 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.009858\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.060878\n",
      "  Validation Loss: 0.006117\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.066740\n",
      "  Validation Loss: 0.007621\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.700404\n",
      "  Validation Loss: 0.500882\n",
      "\n",
      "======== Epoch 4 / 6 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.009290\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.060878\n",
      "  Validation Loss: 0.006117\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.066740\n",
      "  Validation Loss: 0.007621\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.700404\n",
      "  Validation Loss: 0.500882\n",
      "\n",
      "======== Epoch 5 / 6 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.009233\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.060878\n",
      "  Validation Loss: 0.006117\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.066740\n",
      "  Validation Loss: 0.007621\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.700404\n",
      "  Validation Loss: 0.500882\n",
      "\n",
      "======== Epoch 6 / 6 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.009387\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.060878\n",
      "  Validation Loss: 0.006117\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.066740\n",
      "  Validation Loss: 0.007621\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.700404\n",
      "  Validation Loss: 0.500882\n",
      "\n",
      "Training complete!\n",
      "Best metric: 0.7767750108401807\n"
     ]
    }
   ],
   "source": [
    "#Roberta\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import RobertaConfig, RobertaModel, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import RobertaForSequenceClassification\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch \n",
    "from src.data import LexDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from src.training import train, evaluate, forward_func_custom_bert, forward_func_baseline\n",
    "\n",
    "epochs = 6\n",
    "seed_val = 2\n",
    "max_len = 22\n",
    "stop_at = 7\n",
    "cuda_card = 1\n",
    "\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)\n",
    "roberta_results_window = {}\n",
    "\n",
    "for window_x in [1]:\n",
    "    print('Window size of ', window_x)\n",
    "    #data_texts, data_corpus, data_labels, sentence_raw, target_words, positions, _, pos_tags = window_data[window_x]['train']\n",
    "    #news_texts, news_labels, news_target_words, news_positions, news_pos_tags = window_data_cwi[window_x]['news']\n",
    "    #wikipedia_text, wikipedia_labels, wikipedia_target_words, wikipedia_positions, wikipedia_pos_tags = window_data_cwi[window_x]['wikipedia']\n",
    "    #wikinew_texts, wikinew_labels, wikinew_target_words, wikinew_positions, wikinew_pos_tags = window_data_cwi[window_x]['wikinews']\n",
    "    \n",
    "    #pretraining_texts = np.concatenate((news_texts, wikipedia_text, wikinew_texts))\n",
    "    #pretraining_labels = np.concatenate((news_labels, wikipedia_labels, wikinew_labels))\n",
    "    #pretraining_target_words = np.concatenate((news_target_words, wikipedia_target_words, wikinew_target_words))\n",
    "    #pretraining_positions = np.concatenate((news_positions, wikipedia_positions, wikinew_positions))\n",
    "    #pretraining_pos_tags = np.concatenate((news_pos_tags, wikipedia_pos_tags, wikinew_pos_tags))\n",
    "    \n",
    "    train_texts, data_corpus, train_labels, sentence_train, train_target_words, train_positions, _, train_pos_tags = window_data[window_x]['train']\n",
    "    multi_texts, multi_corpus, multi_labels, sentence_multi, multi_target_words, multi_positions, _, multi_pos_tags = window_data[window_x]['train_multi']\n",
    "    data_texts = np.concatenate((train_texts, multi_texts))\n",
    "    data_labels = np.concatenate((train_labels, multi_labels))\n",
    "    sentence_raw = np.concatenate((sentence_train, sentence_multi))\n",
    "    target_words = np.concatenate((train_target_words, multi_target_words))\n",
    "    positions = np.concatenate((train_positions, multi_positions))\n",
    "    pos_tags = np.concatenate((train_pos_tags, multi_pos_tags))\n",
    "    \n",
    "    trial_texts, _, trial_labels, sentences_trial, trial_target_words, trial_positions, _, trial_pos_tags = window_data[window_x]['val']\n",
    "    test_texts, _, test_labels, sentences_test, test_target_words, test_positions, _, test_pos_tags= window_data[window_x]['test']\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    le.fit(pos_tags)\n",
    "    pos_tags_f = le.transform(pos_tags)\n",
    "    trial_pos_tags_f = le.transform(trial_pos_tags)\n",
    "    test_pos_tags_f = le.transform(test_pos_tags)\n",
    "    #pretraining_pos_tags_f = le.transform(pretraining_pos_tags)\n",
    "\n",
    "    pos_tags_f = np.reshape(pos_tags_f, (-1, 1))\n",
    "    trial_pos_tags_f = np.reshape(trial_pos_tags_f, (-1, 1))\n",
    "    test_pos_tags_f = np.reshape(test_pos_tags_f, (-1, 1))\n",
    "    #pretraining_pos_tags_f = np.reshape(pretraining_pos_tags_f, (-1, 1))\n",
    "    \n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    history_occur_text = []\n",
    "    history_occur_target = []\n",
    "    ix_fold = 0\n",
    "\n",
    "    #train_index = list(range(len(pretraining_texts)))\n",
    "    #random.seed(seed_val)\n",
    "    #random.shuffle(train_index)\n",
    "    #train_index = np.array(train_index)\n",
    "    #pretrain_texts_lex, pretrain_labels_lex, pretrain_positions_lex, pretrain_target_words_lex, pretrain_pos_tags = pretraining_texts[train_index], pretraining_labels[train_index], pretraining_positions[train_index], pretraining_target_words[train_index], pretraining_pos_tags_f[train_index]\n",
    "\n",
    "    train_index = list(range(len(data_texts)))\n",
    "    random.seed(seed_val)\n",
    "    random.shuffle(train_index)\n",
    "    train_index = np.array(train_index)\n",
    "\n",
    "    train_texts_lex, train_labels_lex, train_positions_lex, train_target_words_lex, train_pos_tags = data_texts[train_index], data_labels[train_index], positions[train_index], target_words[train_index], pos_tags_f[train_index]\n",
    "\n",
    "    #pretrain_encodings_lex = roberta_tokenizer(list(pretrain_texts_lex), truncation=True, padding=True)\n",
    "    #pretrain_dataset_lex = LexDataset(pretrain_encodings_lex, pretrain_labels_lex, pretrain_positions_lex)\n",
    "    #pretrain_loader_lex = DataLoader(pretrain_dataset_lex, batch_size=8)\n",
    "    \n",
    "    train_encodings_lex = roberta_tokenizer(list(train_texts_lex), truncation=True, padding=True)\n",
    "    train_dataset_lex = LexDataset(train_encodings_lex, train_labels_lex, train_positions_lex)\n",
    "    train_loader_lex = DataLoader(train_dataset_lex, batch_size=32)\n",
    "    \n",
    "    train_encoding_aux = roberta_tokenizer(list(data_texts), truncation=True, padding=True)\n",
    "    train_dataset_aux = LexDataset(train_encoding_aux, data_labels, positions)\n",
    "    train_loader_aux = DataLoader(train_dataset_aux, batch_size=1)\n",
    "    \n",
    "    val_encodings_lex = roberta_tokenizer(list(trial_texts), truncation=True, padding=True)\n",
    "    val_dataset_lex = LexDataset(val_encodings_lex, trial_labels, trial_positions)\n",
    "    val_loader_lex = DataLoader(val_dataset_lex, batch_size=1)\n",
    "    \n",
    "    test_encodings_lex = roberta_tokenizer(list(test_texts), truncation=True, padding=True)\n",
    "    test_dataset_lex = LexDataset(test_encodings_lex, test_labels, test_positions)\n",
    "    test_loader_lex = DataLoader(test_dataset_lex, batch_size=1)\n",
    "        \n",
    "    all_text = ' '.join(data_texts)\n",
    "    occurence_text = [all_text.count(w) for w in trial_target_words]\n",
    "                \n",
    "    device = torch.device(f\"cuda:{cuda_card}\")\n",
    "    #device = torch.device(f\"cpu\")\n",
    "        \n",
    "    model = RobertaForSequenceClassification.from_pretrained(\n",
    "        \"roberta-base\", \n",
    "        num_labels = 1, \n",
    "        output_attentions = False, \n",
    "        output_hidden_states = False, \n",
    "    )\n",
    "    model.cuda(cuda_card)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                      lr = 2e-5, \n",
    "                      eps = 1e-8\n",
    "                      )   \n",
    "    \n",
    "    total_steps = len(train_loader_lex) * 3\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                                num_training_steps = total_steps)\n",
    "    random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "\n",
    "    #for epoch_i in range(0, 3):\n",
    "    #    print()\n",
    "    #    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    #    train(device, model, pretrain_loader_lex, forward_func_baseline, optimizer, scheduler, {})\n",
    "    \n",
    "    #total_steps = len(train_loader_lex) * epochs\n",
    "    #scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "    #                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "    #                                            num_training_steps = total_steps)\n",
    "    \n",
    "    tracking_metrics_trn = []\n",
    "    tracking_logits_trn = []\n",
    "    \n",
    "    tracking_metrics_val = []\n",
    "    tracking_logits_val = []\n",
    "\n",
    "    tracking_metrics_test = []\n",
    "    tracking_logits_test = []\n",
    "        \n",
    "    early_stopping = 0\n",
    "    best_metric = 10000\n",
    "        \n",
    "    for epoch_i in range(0, epochs):\n",
    "        print()\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        train(device, model, train_loader_lex, forward_func_baseline, optimizer, scheduler, {})\n",
    "            \n",
    "        _, trn_logits, trn_metric = evaluate(device, model, train_loader_aux, forward_func_baseline, {})\n",
    "        _, val_logits, val_metric = evaluate(device, model, val_loader_lex, forward_func_baseline, {})\n",
    "        _, test_logits, test_metric = evaluate(device, model, test_loader_lex, forward_func_baseline, {})\n",
    "\n",
    "        tracking_metrics_trn.append(trn_metric)\n",
    "        tracking_logits_trn.append(trn_logits)\n",
    "        \n",
    "        tracking_metrics_val.append(val_metric)\n",
    "        tracking_logits_val.append(val_logits)\n",
    "        \n",
    "        tracking_metrics_test.append(test_metric)\n",
    "        tracking_logits_test.append(test_logits)\n",
    "            \n",
    "        if val_metric > best_metric:\n",
    "            early_stopping += 1\n",
    "        else:\n",
    "            best_metric = val_metric\n",
    "            early_stopping = 0\n",
    "                \n",
    "        if early_stopping == stop_at:\n",
    "            break\n",
    "       \n",
    "    aux = []\n",
    "    for i in range(epochs):\n",
    "        aux.append(pearsonr(tracking_logits_val[i], trial_labels)[0])\n",
    "        \n",
    "    index_min = np.argmax(aux)\n",
    "\n",
    "    ix_fold += 1\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "    print(f\"Best metric: {aux[index_min]}\")\n",
    "        \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "        \n",
    "    results_trn = pd.DataFrame()\n",
    "    results_trn['samples'] = data_texts\n",
    "    results_trn['preds'] = tracking_logits_trn[index_min]\n",
    "    results_trn['true'] = data_labels\n",
    "    \n",
    "    results_val = pd.DataFrame()\n",
    "    results_val['samples'] = trial_texts\n",
    "    results_val['preds'] = tracking_logits_val[index_min]\n",
    "    results_val['true'] = trial_labels\n",
    "    \n",
    "    results_test = pd.DataFrame()\n",
    "    results_test['samples'] = test_texts\n",
    "    results_test['preds'] = tracking_logits_test[index_min]\n",
    "    results_test['true'] = test_labels\n",
    "\n",
    "    roberta_results_window[window_x] = {'train': [], 'val': [], 'test': []}\n",
    "    roberta_results_window[window_x]['train'] = results_trn    \n",
    "    roberta_results_window[window_x]['val'] = results_val\n",
    "    roberta_results_window[window_x]['test'] = results_test    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6903768286718371, 8.71519239679736e-131)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['preds'] = roberta_results_window[window_x]['test'].preds\n",
    "df_cross = pd.merge(df_test, df_true, on=['id'])\n",
    "\n",
    "df_cross[['id', 'preds']].to_csv('submit_1.csv', index=False, header=None)\n",
    "pearsonr(df_cross.preds, df_cross.complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window size of  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.022626\n",
      "\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.016170\n",
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.015601\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.067958\n",
      "  Validation Loss: 0.007753\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.711809\n",
      "  Validation Loss: 0.514285\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.230103\n",
      "  Validation Loss: 0.062778\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.010453\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.067120\n",
      "  Validation Loss: 0.007433\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.693769\n",
      "  Validation Loss: 0.493569\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.238676\n",
      "  Validation Loss: 0.069735\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.009295\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.068336\n",
      "  Validation Loss: 0.007820\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.682838\n",
      "  Validation Loss: 0.478538\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.247697\n",
      "  Validation Loss: 0.073932\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.008076\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.067883\n",
      "  Validation Loss: 0.007997\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.683113\n",
      "  Validation Loss: 0.479622\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.248374\n",
      "  Validation Loss: 0.074748\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.007534\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.063557\n",
      "  Validation Loss: 0.006991\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.703961\n",
      "  Validation Loss: 0.506994\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.234469\n",
      "  Validation Loss: 0.067394\n",
      "\n",
      "Training complete!\n",
      "Best metric: 0.7970162257359438\n",
      "=========================\n",
      "Preliminary results: (0.736343026795155, 2.0528055578848728e-157)\n",
      "\n",
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.024192\n",
      "\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.024207\n",
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.009924\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.067632\n",
      "  Validation Loss: 0.008122\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.695216\n",
      "  Validation Loss: 0.496934\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.134881\n",
      "  Validation Loss: 0.023036\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.007348\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.068831\n",
      "  Validation Loss: 0.008128\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.688974\n",
      "  Validation Loss: 0.489258\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.142103\n",
      "  Validation Loss: 0.025334\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.006576\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.066300\n",
      "  Validation Loss: 0.007585\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.697412\n",
      "  Validation Loss: 0.498826\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.138034\n",
      "  Validation Loss: 0.023843\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.006133\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.067533\n",
      "  Validation Loss: 0.007780\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.697702\n",
      "  Validation Loss: 0.500970\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.136087\n",
      "  Validation Loss: 0.023614\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.005751\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.065096\n",
      "  Validation Loss: 0.007184\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.707469\n",
      "  Validation Loss: 0.512349\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.130397\n",
      "  Validation Loss: 0.021469\n",
      "\n",
      "Training complete!\n",
      "Best metric: 0.7916488797161022\n",
      "=========================\n",
      "Preliminary results: (0.7446917077028818, 7.366476937367419e-163)\n",
      "\n",
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.023846\n",
      "\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.023738\n",
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.007676\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.067009\n",
      "  Validation Loss: 0.007856\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.694352\n",
      "  Validation Loss: 0.495366\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.139576\n",
      "  Validation Loss: 0.023980\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.005768\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.066650\n",
      "  Validation Loss: 0.007935\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.700310\n",
      "  Validation Loss: 0.504183\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.133429\n",
      "  Validation Loss: 0.022373\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.005225\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.067242\n",
      "  Validation Loss: 0.008022\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.692145\n",
      "  Validation Loss: 0.492678\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.140256\n",
      "  Validation Loss: 0.024358\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.004962\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.065895\n",
      "  Validation Loss: 0.007650\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.698657\n",
      "  Validation Loss: 0.500489\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.137539\n",
      "  Validation Loss: 0.023443\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.004826\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.065835\n",
      "  Validation Loss: 0.007461\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.708256\n",
      "  Validation Loss: 0.514127\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.127532\n",
      "  Validation Loss: 0.020710\n",
      "\n",
      "Training complete!\n",
      "Best metric: 0.7908509845861712\n",
      "=========================\n",
      "Preliminary results: (0.7400552278939332, 8.263070049470091e-160)\n",
      "\n",
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.023590\n",
      "\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.023521\n",
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.005937\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.069268\n",
      "  Validation Loss: 0.008315\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.696290\n",
      "  Validation Loss: 0.499895\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.135045\n",
      "  Validation Loss: 0.023508\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.004785\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.069437\n",
      "  Validation Loss: 0.008328\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.691350\n",
      "  Validation Loss: 0.491804\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.138588\n",
      "  Validation Loss: 0.023963\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.004377\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.070742\n",
      "  Validation Loss: 0.008590\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.698086\n",
      "  Validation Loss: 0.502874\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.133272\n",
      "  Validation Loss: 0.023506\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.004376\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.068389\n",
      "  Validation Loss: 0.007960\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.698353\n",
      "  Validation Loss: 0.501186\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.133688\n",
      "  Validation Loss: 0.022741\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.004096\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.067389\n",
      "  Validation Loss: 0.007604\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.706295\n",
      "  Validation Loss: 0.511725\n",
      "\n",
      "Running Validation...\n",
      "  Metric: 0.127409\n",
      "  Validation Loss: 0.021033\n",
      "\n",
      "Training complete!\n",
      "Best metric: 0.7896438299946753\n",
      "=========================\n",
      "Preliminary results: (0.7411180326057657, 1.6742373744273903e-160)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "seed_val = 2\n",
    "max_len = 22\n",
    "stop_at = 7\n",
    "cuda_card = 1\n",
    "\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)\n",
    "roberta_pretrain_results_window = {}\n",
    "\n",
    "for window_x in [1]:\n",
    "    print('Window size of ', window_x)\n",
    "    data_texts, data_corpus, data_labels, sentence_raw, target_words, positions, _, pos_tags = window_data[window_x]['train']\n",
    "    news_texts, news_labels, news_target_words, news_positions, news_pos_tags = window_data_cwi[window_x]['news']\n",
    "    wikipedia_text, wikipedia_labels, wikipedia_target_words, wikipedia_positions, wikipedia_pos_tags = window_data_cwi[window_x]['wikipedia']\n",
    "    wikinew_texts, wikinew_labels, wikinew_target_words, wikinew_positions, wikinew_pos_tags = window_data_cwi[window_x]['wikinews']\n",
    "    \n",
    "    pretraining_texts = np.concatenate((news_texts, wikipedia_text, wikinew_texts))\n",
    "    pretraining_labels = np.concatenate((news_labels, wikipedia_labels, wikinew_labels))\n",
    "    pretraining_target_words = np.concatenate((news_target_words, wikipedia_target_words, wikinew_target_words))\n",
    "    pretraining_positions = np.concatenate((news_positions, wikipedia_positions, wikinew_positions))\n",
    "    pretraining_pos_tags = np.concatenate((news_pos_tags, wikipedia_pos_tags, wikinew_pos_tags))\n",
    "    \n",
    "    train_texts, data_corpus, train_labels, sentence_train, train_target_words, train_positions, _, train_pos_tags = window_data[window_x]['train']\n",
    "    multi_texts, multi_corpus, multi_labels, sentence_multi, multi_target_words, multi_positions, _, multi_pos_tags = window_data[window_x]['train_multi']\n",
    "    data_texts = np.concatenate((train_texts, multi_texts))\n",
    "    data_labels = np.concatenate((train_labels, multi_labels))\n",
    "    sentence_raw = np.concatenate((sentence_train, sentence_multi))\n",
    "    target_words = np.concatenate((train_target_words, multi_target_words))\n",
    "    positions = np.concatenate((train_positions, multi_positions))\n",
    "    pos_tags = np.concatenate((train_pos_tags, multi_pos_tags))\n",
    "    \n",
    "    trial_texts, _, trial_labels, sentences_trial, trial_target_words, trial_positions, _, trial_pos_tags = window_data[window_x]['val']\n",
    "    test_texts, _, test_labels, sentences_test, test_target_words, test_positions, _, test_pos_tags= window_data[window_x]['test']\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    le.fit(pretraining_pos_tags)\n",
    "    pos_tags_f = le.transform(pos_tags)\n",
    "    trial_pos_tags_f = le.transform(trial_pos_tags)\n",
    "    test_pos_tags_f = le.transform(test_pos_tags)\n",
    "    pretraining_pos_tags_f = le.transform(pretraining_pos_tags)\n",
    "\n",
    "    pos_tags_f = np.reshape(pos_tags_f, (-1, 1))\n",
    "    trial_pos_tags_f = np.reshape(trial_pos_tags_f, (-1, 1))\n",
    "    test_pos_tags_f = np.reshape(test_pos_tags_f, (-1, 1))\n",
    "    pretraining_pos_tags_f = np.reshape(pretraining_pos_tags_f, (-1, 1))\n",
    "    \n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    history_occur_text = []\n",
    "    history_occur_target = []\n",
    "    ix_fold = 0\n",
    "\n",
    "    train_index = list(range(len(pretraining_texts)))\n",
    "    random.seed(seed_val)\n",
    "    random.shuffle(train_index)\n",
    "    train_index = np.array(train_index)\n",
    "    pretrain_texts_lex, pretrain_labels_lex, pretrain_positions_lex, pretrain_target_words_lex, pretrain_pos_tags = pretraining_texts[train_index], pretraining_labels[train_index], pretraining_positions[train_index], pretraining_target_words[train_index], pretraining_pos_tags_f[train_index]\n",
    "\n",
    "    train_index = list(range(len(data_texts)))\n",
    "    random.seed(seed_val)\n",
    "    random.shuffle(train_index)\n",
    "    train_index = np.array(train_index)\n",
    "\n",
    "    train_texts_lex, train_labels_lex, train_positions_lex, train_target_words_lex, train_pos_tags = data_texts[train_index], data_labels[train_index], positions[train_index], target_words[train_index], pos_tags_f[train_index]\n",
    "    \n",
    "    pretrain_encodings_lex = roberta_tokenizer(list(pretrain_texts_lex), truncation=True, padding=True)\n",
    "    \n",
    "    train_encodings_lex = roberta_tokenizer(list(train_texts_lex), truncation=True, padding=True)\n",
    "    train_dataset_lex = LexDataset(train_encodings_lex, train_labels_lex, train_positions_lex)\n",
    "    train_loader_lex = DataLoader(train_dataset_lex, batch_size=32)\n",
    "    \n",
    "    train_encoding_aux = roberta_tokenizer(list(data_texts), truncation=True, padding=True)\n",
    "    train_dataset_aux = LexDataset(train_encoding_aux, data_labels, positions)\n",
    "    train_loader_aux = DataLoader(train_dataset_aux, batch_size=1)\n",
    "    \n",
    "    val_encodings_lex = roberta_tokenizer(list(trial_texts), truncation=True, padding=True)\n",
    "    val_dataset_lex = LexDataset(val_encodings_lex, trial_labels, trial_positions)\n",
    "    val_loader_lex = DataLoader(val_dataset_lex, batch_size=1)\n",
    "    \n",
    "    test_encodings_lex = roberta_tokenizer(list(test_texts), truncation=True, padding=True)\n",
    "    test_dataset_lex = LexDataset(test_encodings_lex, test_labels, test_positions)\n",
    "    test_loader_lex = DataLoader(test_dataset_lex, batch_size=1)\n",
    "        \n",
    "    all_text = ' '.join(data_texts)\n",
    "    occurence_text = [all_text.count(w) for w in trial_target_words]\n",
    "                \n",
    "    device = torch.device(f\"cuda:{cuda_card}\")\n",
    "    #device = torch.device(f\"cpu\")\n",
    "        \n",
    "    model = RobertaForSequenceClassification.from_pretrained(\n",
    "        \"roberta-base\", \n",
    "        num_labels = 1, \n",
    "        output_attentions = False, \n",
    "        output_hidden_states = False, \n",
    "    )\n",
    "    model.cuda(cuda_card)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                      lr = 2e-5, \n",
    "                      eps = 1e-8\n",
    "                      )   \n",
    "    \n",
    "    total_steps = len(train_loader_lex) * 3\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                                num_training_steps = total_steps)\n",
    "    random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "\n",
    "    for passing_id in range(4):\n",
    "        roberta_pretrain_results_window[passing_id] = {}\n",
    "\n",
    "        if passing_id:\n",
    "            pretrain_labels_final = pretrain_labels_lex * 0.6 + np.array(tracking_pretrain_logits_test[index_min]) * 0.4\n",
    "        else:\n",
    "            pretrain_labels_final = pretrain_labels_lex\n",
    "        \n",
    "        pretrain_dataset_lex = LexDataset(pretrain_encodings_lex, pretrain_labels_final, pretrain_positions_lex)\n",
    "        pretrain_loader_lex = DataLoader(pretrain_dataset_lex, batch_size=24)\n",
    "        pretrain_pred_loader_lex = DataLoader(pretrain_dataset_lex, batch_size=1)\n",
    "        \n",
    "        for epoch_i in range(0, 2):\n",
    "            print()\n",
    "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, 2))\n",
    "            train(device, model, pretrain_loader_lex, forward_func_baseline, optimizer, scheduler, {})\n",
    "\n",
    "        total_steps = len(train_loader_lex) * epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                    num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                                    num_training_steps = total_steps)\n",
    "\n",
    "        tracking_metrics_trn = []\n",
    "        tracking_logits_trn = []\n",
    "\n",
    "        tracking_metrics_val = []\n",
    "        tracking_logits_val = []\n",
    "\n",
    "        tracking_metrics_test = []\n",
    "        tracking_logits_test = []\n",
    "        \n",
    "        tracking_pretrain_metrics_test = []\n",
    "        tracking_pretrain_logits_test = []\n",
    "\n",
    "        early_stopping = 0\n",
    "        best_metric = 10000\n",
    "\n",
    "        for epoch_i in range(0, epochs):\n",
    "            print()\n",
    "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "            train(device, model, train_loader_lex, forward_func_baseline, optimizer, scheduler, {})\n",
    "\n",
    "            #_, trn_logits, trn_metric = evaluate(device, model, train_loader_aux, forward_func_baseline, {})\n",
    "            _, val_logits, val_metric = evaluate(device, model, val_loader_lex, forward_func_baseline, {})\n",
    "            _, test_logits, test_metric = evaluate(device, model, test_loader_lex, forward_func_baseline, {})\n",
    "            _, pretrain_logits, pretrain_metric = evaluate(device, model, pretrain_pred_loader_lex, forward_func_baseline, {})\n",
    "\n",
    "            #tracking_metrics_trn.append(trn_metric)\n",
    "            #tracking_logits_trn.append(trn_logits)\n",
    "\n",
    "            tracking_metrics_val.append(val_metric)\n",
    "            tracking_logits_val.append(val_logits)\n",
    "\n",
    "            tracking_metrics_test.append(test_metric)\n",
    "            tracking_logits_test.append(test_logits)\n",
    "            \n",
    "            tracking_pretrain_metrics_test.append(pretrain_metric)\n",
    "            tracking_pretrain_logits_test.append(pretrain_logits)\n",
    "\n",
    "            if val_metric > best_metric:\n",
    "                early_stopping += 1\n",
    "            else:\n",
    "                best_metric = val_metric\n",
    "                early_stopping = 0\n",
    "\n",
    "            if early_stopping == stop_at:\n",
    "                break\n",
    "\n",
    "        aux = []\n",
    "        for i in range(epochs):\n",
    "            aux.append(pearsonr(tracking_logits_val[i], trial_labels)[0])\n",
    "\n",
    "        index_min = np.argmax(aux)\n",
    "\n",
    "        ix_fold += 1\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Training complete!\")\n",
    "        print(f\"Best metric: {aux[index_min]}\")\n",
    "\n",
    "        #torch.cuda.empty_cache()\n",
    "\n",
    "        results_trn = pd.DataFrame()\n",
    "        results_trn['samples'] = data_texts\n",
    "        results_trn['preds'] = 1#tracking_logits_trn[index_min]\n",
    "        results_trn['true'] = data_labels\n",
    "\n",
    "        results_val = pd.DataFrame()\n",
    "        results_val['samples'] = trial_texts\n",
    "        results_val['preds'] = tracking_logits_val[index_min]\n",
    "        results_val['true'] = trial_labels\n",
    "\n",
    "        results_test = pd.DataFrame()\n",
    "        results_test['samples'] = test_texts\n",
    "        results_test['preds'] = tracking_logits_test[index_min]\n",
    "        results_test['true'] = test_labels\n",
    "        \n",
    "        roberta_pretrain_results_window[passing_id][window_x] = {'train': [], 'val': [], 'test': []}\n",
    "        roberta_pretrain_results_window[passing_id][window_x]['train'] = results_trn    \n",
    "        roberta_pretrain_results_window[passing_id][window_x]['val'] = results_val\n",
    "        roberta_pretrain_results_window[passing_id][window_x]['test'] = results_test    \n",
    "        \n",
    "        df_test['preds'] = roberta_pretrain_results_window[passing_id][window_x]['test'].preds\n",
    "        df_cross = pd.merge(df_test, df_true, on=['id'])\n",
    "        \n",
    "        print('=========================')\n",
    "        print('Preliminary results:', pearsonr(df_cross.preds, df_cross.complexity))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7411180326057657, 1.6742373744273903e-160)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(df_cross.preds, df_cross.complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "Preliminary results: (0.736343026795155, 2.0528055578848728e-157)\n",
      "=========================\n",
      "Preliminary results: (0.7446917077028818, 7.366476937367419e-163)\n",
      "=========================\n",
      "Preliminary results: (0.7400552278939332, 8.263070049470091e-160)\n",
      "=========================\n",
      "Preliminary results: (0.7411180326057657, 1.6742373744273903e-160)\n"
     ]
    }
   ],
   "source": [
    "for passing_id in range(4):\n",
    "    df_test['preds'] = roberta_pretrain_results_window[passing_id][window_x]['test'].preds\n",
    "    df_cross = pd.merge(df_test, df_true, on=['id'])\n",
    "        \n",
    "    print('=========================')\n",
    "    print('Preliminary results:', pearsonr(df_cross.preds, df_cross.complexity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
